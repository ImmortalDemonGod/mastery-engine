{
  "curriculum_name": "cs336_a1",
  "author": "CS336 Teaching Team (Adapted for Mastery Engine)",
  "version": "2.0.0",
  "description": "Complete from-scratch Transformer LM implementation with 21 modules in dependency order",
  "modules": [
    {
      "id": "softmax",
      "name": "Numerically Stable Softmax",
      "path": "modules/softmax",
      "baseline_perf_seconds": 0.15,
      "dependencies": []
    },
    {
      "id": "cross_entropy",
      "name": "Numerically Stable Cross-Entropy Loss",
      "path": "modules/cross_entropy",
      "baseline_perf_seconds": 0.15,
      "dependencies": ["softmax"]
    },
    {
      "id": "gradient_clipping",
      "name": "Global Gradient Clipping",
      "path": "modules/gradient_clipping",
      "baseline_perf_seconds": 0.10,
      "dependencies": []
    },
    {
      "id": "linear",
      "name": "Linear (Fully-Connected) Layer",
      "path": "modules/linear",
      "baseline_perf_seconds": 0.12,
      "dependencies": []
    },
    {
      "id": "embedding",
      "name": "Token Embedding Layer (From Scratch)",
      "path": "modules/embedding",
      "baseline_perf_seconds": 0.10,
      "dependencies": []
    },
    {
      "id": "silu",
      "name": "SiLU (Sigmoid Linear Unit) Activation",
      "path": "modules/silu",
      "baseline_perf_seconds": 0.08,
      "dependencies": []
    },
    {
      "id": "rmsnorm",
      "name": "Root Mean Square Layer Normalization",
      "path": "modules/rmsnorm",
      "baseline_perf_seconds": 0.12,
      "dependencies": []
    },
    {
      "id": "swiglu",
      "name": "SwiGLU Gated Feed-Forward Network",
      "path": "modules/swiglu",
      "baseline_perf_seconds": 0.15,
      "dependencies": ["linear", "silu"]
    },
    {
      "id": "attention",
      "name": "Scaled Dot-Product Attention",
      "path": "modules/attention",
      "baseline_perf_seconds": 0.18,
      "dependencies": ["softmax"]
    },
    {
      "id": "rope",
      "name": "RoPE (Rotary Position Embeddings)",
      "path": "modules/rope",
      "baseline_perf_seconds": 0.14,
      "dependencies": []
    },
    {
      "id": "multihead_attention",
      "name": "Multi-Head Self-Attention with RoPE",
      "path": "modules/multihead_attention",
      "baseline_perf_seconds": 0.22,
      "dependencies": ["attention", "rope", "linear"]
    },
    {
      "id": "transformer_block",
      "name": "Transformer Block (Complete Integration)",
      "path": "modules/transformer_block",
      "baseline_perf_seconds": 0.25,
      "dependencies": ["multihead_attention", "swiglu", "rmsnorm"]
    },
    {
      "id": "transformer_lm",
      "name": "Transformer Language Model Assembly",
      "path": "modules/transformer_lm",
      "baseline_perf_seconds": 0.30,
      "dependencies": ["transformer_block", "embedding", "rmsnorm", "linear"]
    },
    {
      "id": "adamw",
      "name": "AdamW Optimizer",
      "path": "modules/adamw",
      "baseline_perf_seconds": 0.15,
      "dependencies": []
    },
    {
      "id": "cosine_schedule",
      "name": "Cosine Learning Rate Schedule with Warmup",
      "path": "modules/cosine_schedule",
      "baseline_perf_seconds": 0.08,
      "dependencies": []
    },
    {
      "id": "data_loader",
      "name": "Language Model Data Loader (get_batch)",
      "path": "modules/data_loader",
      "baseline_perf_seconds": 0.12,
      "dependencies": []
    },
    {
      "id": "checkpointing",
      "name": "Model Checkpointing (Save/Load)",
      "path": "modules/checkpointing",
      "baseline_perf_seconds": 0.15,
      "dependencies": []
    },
    {
      "id": "training_loop",
      "name": "Training Loop (Complete Integration)",
      "path": "modules/training_loop",
      "baseline_perf_seconds": 0.20,
      "dependencies": ["transformer_lm", "adamw", "cosine_schedule", "data_loader", "checkpointing", "cross_entropy", "gradient_clipping"]
    },
    {
      "id": "unicode",
      "name": "Unicode and Text Encoding Foundations",
      "path": "modules/unicode",
      "baseline_perf_seconds": 0.15,
      "dependencies": [],
      "module_type": "justify_only"
    },
    {
      "id": "bpe_tokenizer",
      "name": "BPE Tokenizer Training",
      "path": "modules/bpe_tokenizer",
      "baseline_perf_seconds": 0.18,
      "dependencies": ["unicode"]
    },
    {
      "id": "tokenizer_class",
      "name": "Tokenizer Class (Encode/Decode)",
      "path": "modules/tokenizer_class",
      "baseline_perf_seconds": 0.12,
      "dependencies": ["bpe_tokenizer"]
    },
    {
      "id": "text_generation",
      "name": "Text Generation with Sampling Strategies",
      "path": "modules/text_generation",
      "baseline_perf_seconds": 0.25,
      "dependencies": ["transformer_lm", "tokenizer_class"]
    }
  ]
}
