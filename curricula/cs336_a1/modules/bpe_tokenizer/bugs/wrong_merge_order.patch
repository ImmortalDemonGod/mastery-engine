--- a/cs336_basics/tokenizer.py
+++ b/cs336_basics/tokenizer.py
@@ -45,8 +45,10 @@ def train_bpe(text, vocab_size):
         # Find most frequent pair
         best_pair = max(pair_counts, key=pair_counts.get)
         
-        # Record merge (in order!)
-        merges.append((vocab[best_pair[0]], vocab[best_pair[1]]))
+        # BUG: Recording merges in wrong order! Inserting at beginning instead of appending.
+        # This reverses merge order, breaking encoding since later merges depend on earlier ones.
+        # Should be: merges.append(...)
+        merges.insert(0, (vocab[best_pair[0]], vocab[best_pair[1]]))  # Wrong!
         
         # Create new token
         vocab[token_id] = vocab[best_pair[0]] + vocab[best_pair[1]]
