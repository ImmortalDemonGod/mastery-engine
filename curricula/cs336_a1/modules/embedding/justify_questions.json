[
  {
    "id": "embedding_q1",
    "question": "Embeddings convert discrete token IDs to continuous vectors, enabling gradient-based learning. Explain why neural networks cannot directly operate on discrete token IDs (integers), using the concept of differentiability and the chain rule. What breaks in backpropagation without continuous representations?",
    "model_answer": "Neural networks require differentiability for gradient-based learning, which discrete representations fundamentally lack. Why discrete fails: (1) No gradients: If input is integer token ID t=42, ∂loss/∂t is undefined. Integers don't have gradients in the real number sense—you can't have 'slightly more than 42' in a meaningful way for discrete symbols. (2) Chain rule breaks: Backprop uses chain rule: ∂L/∂θ = ∂L/∂x × ∂x/∂θ. If x is discrete (token ID), ∂x/∂θ is undefined for any parameter θ. Can't compute how changing parameters affects discrete outputs. (3) No smooth optimization landscape: Gradient descent requires smooth loss landscape—small parameter changes → small loss changes. Discrete inputs create discontinuous loss: changing parameter slightly might not change which token is closest, then suddenly jump to different token. (4) Can't interpolate: Neural networks learn by finding patterns in continuous space. Can't take 'average' of tokens 42 and 87 in meaningful way with integers. With continuous embeddings: (1) Gradients exist: Embedding vector e ∈ ℝ^d has well-defined gradients ∂L/∂e_i for each dimension. (2) Chain rule works: ∂L/∂E[t] = ∂L/∂e × ∂e/∂E[t] where e = E[t]. Even though lookup is discrete, the embedding values are continuous parameters. (3) Smooth space: Small changes to embedding E[t] cause small changes to loss, enabling gradient descent. (4) Semantic interpolation: Can compute meaningful relationships like (E[king] - E[man] + E[woman]) in continuous space. The key insight: Lookup operation is discrete (∂t/∂θ undefined), but lookup result E[t] is continuous (∂E[t]/∂θ defined), allowing gradients to flow to embedding parameters.",
    "required_concepts": [
      "Discrete values have no gradients (∂t/∂θ undefined)",
      "Chain rule requires differentiability at each step",
      "Gradient descent needs smooth, continuous landscape",
      "Embeddings are continuous parameters despite discrete lookup",
      "Gradients flow to embedding values E[t], not to index t",
      "Continuous space enables semantic interpolation"
    ]
  },
  {
    "id": "embedding_q2",
    "question": "After training, embeddings for semantically similar tokens (e.g., 'cat' and 'dog') end up close in vector space, despite random initialization. Explain the learning mechanism that creates this semantic structure through language modeling, without any explicit supervision about semantic similarity.",
    "model_answer": "Semantic structure emerges implicitly from language modeling's distributional hypothesis: words appearing in similar contexts should have similar meanings. Learning mechanism: (1) Language modeling objective: Predict next token given context. Loss = -log P(token | context). Model must learn which tokens are likely in which contexts. (2) Context similarity creates embedding similarity: Consider contexts like 'The ___ is sleeping' or 'I pet the ___'. Both 'cat' and 'dog' appear frequently in these contexts. To minimize loss, model learns: P(cat | 'The ___ is sleeping') should be high, P(dog | 'The ___ is sleeping') should be high. (3) Shared contexts push embeddings together: For similar predictions, the model wants similar hidden states h. Since h = f(embeddings, previous_tokens), similar required predictions push embeddings to be similar. If E[cat] and E[dog] are very different but need same predictions, there's conflict—model must work harder. If E[cat] ≈ E[dog], transformations are easier, loss is lower. (4) Gradient descent discovers this: Gradients push E[cat] and E[dog] toward configurations that make prediction easier in their typical contexts. Since they share contexts, they're pushed in similar directions. (5) Contrastive learning (implicit): 'cat' should predict similar next tokens as 'dog' (common: 'meowed', 'purred', 'slept') but different from 'car' (common: 'drove', 'crashed'). This creates clustering: animal embeddings cluster together, vehicle embeddings cluster together. No explicit 'cat is similar to dog' supervision needed—it emerges from statistical patterns in language! This is distributional semantics: 'You shall know a word by the company it keeps' (Firth, 1957).",
    "required_concepts": [
      "Language modeling: predict tokens from context",
      "Distributional hypothesis: similar contexts → similar meanings",
      "Shared contexts require similar predictions",
      "Similar predictions benefit from similar embeddings",
      "Gradient descent pushes related embeddings together",
      "Semantic structure emerges from statistical patterns"
    ]
  },
  {
    "id": "embedding_q3",
    "question": "Tied embeddings use the same weight matrix for input embeddings (token→vector) and output projection (vector→logits): E_in = W_out^T. Explain why this works despite the tasks being inverses (input: lookup, output: dot product over vocab). Why doesn't this create a conflict?",
    "model_answer": "Tied embeddings work because both tasks benefit from the same semantic space representation, even though operations differ. Input embedding E_in: Maps token t to vector e = E_in[t]. Task: 'What does this token mean?' Learn: Rows of E_in should represent token semantics in d_model dimensional space. Output projection W_out: Maps hidden state h to logits l = h @ W_out. Task: 'Which token should come next?' W_out has shape (d_model, vocab_size), so W_out^T has shape (vocab_size, d_model)—same as E_in! Each row of W_out^T represents a token in d_model space. Why no conflict: (1) Shared semantic space: Both need same thing—good token representations. Input: E[dog] should encode 'animal, pet, mammal' meaning. Output: W_out^T[dog] should be activated by hidden states representing 'animal-like' predictions. These are aligned! If hidden state h represents 'animal context', then h @ W_out gives high logit for 'dog' because W_out^T[dog] (the dog row) has high dot product with 'animal' h. (2) Different operations, same semantics: Input: E[t] directly retrieves token's semantic vector. Output: h @ W_out^T[t] scores how well hidden state h matches token t's semantics. These are complementary: If E[dog] = 'animal semantic vector', then scoring h against it checks 'does h represent animal context?'. (3) Regularization benefit: Tying forces consistency: If E[dog] encodes certain semantics, output must use those same semantics. Prevents drift where input and output learn different representations for same token. (4) Parameter efficiency: Instead of learning two separate vocab_size × d_model matrices, learn one. Regularizes by forcing agreement on token meanings. The insight: Both tasks need semantic representations; sharing them enforces consistency and saves parameters without hurting (and often helping) performance!",
    "required_concepts": [
      "Input embedding: token → semantic vector",
      "Output projection: hidden state → similarity scores",
      "Both need same semantic space representation",
      "Operations differ but semantic goals align",
      "Dot product h @ W[t] scores semantic match",
      "Tying enforces consistency, saves parameters"
    ]
  },
  {
    "id": "embedding_q4",
    "question": "In GPT-2 small, embeddings comprise 31% of the 124M total parameters (38.6M for 50,257 tokens × 768 dims). Explain why embeddings scale linearly with vocabulary size while Transformer layers scale quadratically with model dimension. How does this affect scaling strategies for large models?",
    "model_answer": "Scaling properties differ fundamentally between embeddings and Transformer layers: Embeddings: Parameters = vocab_size × d_model. Linear in both vocab_size and d_model. For GPT-2: 50,257 × 768 = 38.6M. If double vocab: 100,000 × 768 = 76.8M (doubles). If double d_model: 50,257 × 1536 = 77.2M (doubles). Linear scaling! Transformer layers (per layer): Attention: 4 × d_model² (Q,K,V,O projections, each d_model × d_model). FFN: ~3 × d_model × d_ff where d_ff ≈ (8/3) × d_model ≈ 8 × d_model². Total per layer: ~12 × d_model². If double d_model: 12 × (2d)² = 12 × 4d² = 4× parameters! Quadratic scaling! Full model with L layers: Embeddings: vocab × d. Transformer: L × 12 × d². Total: vocab × d + L × 12 × d². Impact on scaling strategies: (1) Embeddings dominate for small d_model: At d=768, embeddings are 31% (GPT-2). At d=256, embeddings would be even higher percentage. For small models, vocab is bottleneck. (2) Transformer dominates for large d_model: GPT-3 (d=12,288): Embeddings = 50K × 12K ≈ 600M. Transformer = 96 layers × 12 × (12K)² ≈ 166B. Embeddings are only 0.36% of total! Quadratic growth dominates. (3) Strategies for large models: Vocabulary: Keep vocab reasonable (30K-100K), don't grow indefinitely. Use subword tokenization (BPE) for compact vocab. Tied embeddings save 50% of embedding parameters. Model dimension: This is where capacity comes from! Scaling d from 4K to 12K gives 9× more transformer parameters. Focus scaling here. (4) Factorized embeddings: For extreme cases, use E = A × B (lower rank) to reduce embedding parameters further. These different scaling properties mean large models should prioritize depth and dimension over vocabulary size!",
    "required_concepts": [
      "Embeddings: O(vocab × d) - linear scaling",
      "Transformer layers: O(d²) - quadratic scaling",
      "Small models: embeddings significant fraction",
      "Large models: transformer dominates (quadratic)",
      "Scaling strategy: grow d and L, not vocab",
      "Tied/factorized embeddings save embedding parameters"
    ]
  },
  {
    "id": "embedding_q5",
    "question": "Embedding lookup can be viewed as one-hot encoding followed by matrix multiplication: E^T × one_hot(t) = E[t]. Explain why we don't actually create one-hot vectors in practice, using computational efficiency arguments about sparse vs dense operations and memory bandwidth.",
    "model_answer": "One-hot formulation is mathematically equivalent but computationally wasteful compared to direct indexing. One-hot approach: (1) Create one-hot vector x ∈ ℝ^vocab_size, all zeros except x[t]=1. Memory: vocab_size floats (e.g., 50K tokens = 200KB per token with float32). For batch_size=32, seq_len=512: 32 × 512 × 50K = 819M floats = 3.2GB just for one-hot vectors! (2) Matrix multiply: y = E^T @ x where E^T is (d_model × vocab_size). Operations: d_model × vocab_size multiplications (e.g., 768 × 50K = 38M ops). Even though result is sparse (only one column used), must compute full matrix-vector product. (3) Memory bandwidth: Must load entire E^T (38M parameters) and entire x (50K floats) from memory. Memory access is often the bottleneck, not compute. Direct indexing approach: (1) No one-hot vector creation: Memory: zero extra, just store token IDs (integers, 4 bytes each). For batch=32, seq=512: 32 × 512 × 4B = 64KB (vs 3.2GB!). (2) Direct array lookup: result = E[t] (single row extraction). Operations: zero multiplications, just memory access. Copy d_model floats from memory. (3) Memory bandwidth: Load only the needed row of E (768 floats = 3KB), not entire matrix. 10,000× less memory access! Computational savings: One-hot: O(vocab_size × d_model) operations + O(vocab_size) memory per token. Direct: O(1) operations + O(d_model) memory per token. For typical sizes, direct indexing is 1000-10,000× faster and uses 100-1000× less memory! This is why every deep learning framework uses direct indexing—sparse operations on explicit one-hot vectors is extraordinarily wasteful. The mathematical view helps understanding, but implementation is pure indexing.",
    "required_concepts": [
      "One-hot: sparse vector, vocab_size dimensions",
      "Matrix multiply with one-hot: wasteful, touches all weights",
      "Direct indexing: just copy one row, O(1) operation",
      "Memory bandwidth: one-hot loads full matrix, indexing loads one row",
      "One-hot uses 1000× more memory",
      "Direct indexing 1000-10,000× faster",
      "Mathematical equivalence ≠ computational equivalence"
    ]
  }
]
