--- a/cs336_basics/layers.py
+++ b/cs336_basics/layers.py
@@ -285,10 +285,11 @@ def multihead_self_attention_with_rope(
     # All heads computed in parallel
     attn_output = scaled_dot_product_attention(Q_rope, K_rope, V, mask=None)
     
-    # Transpose back: (..., num_heads, seq_len, d_head) → (..., seq_len, num_heads, d_head)
-    attn_output = attn_output.transpose(-3, -2)
+    # BUG: Missing transpose back before reshaping!
+    # Without this, we concatenate heads in wrong order, mixing information incorrectly.
+    # Should be: attn_output = attn_output.transpose(-3, -2)
     
-    # Concatenate heads: (..., seq_len, num_heads, d_head) → (..., seq_len, d_model)
+    # Concatenate heads: Wrong shape for concat!
     attn_output = attn_output.reshape(*batch_shape, seq_len, d_model)
     
     # Output projection
