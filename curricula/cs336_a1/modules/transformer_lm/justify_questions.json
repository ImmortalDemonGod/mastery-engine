[
  {
    "id": "transformer_lm_q1",
    "question": "During training, the loss is computed as CrossEntropy(logits[:, :-1], targets[:, 1:]). Explain why we shift targets by one position and drop the last logit. What would happen if we used CrossEntropy(logits, targets) directly?",
    "model_answer": "The shift implements autoregressive next-token prediction. Why we shift: Autoregressive modeling means: predict token at position t using ONLY tokens 0..t-1. Training setup: Input sequence x = [x₀, x₁, x₂, x₃]. Targets y = [x₁, x₂, x₃, x₄] (shifted by 1). Model forward: logits = model(x) produces predictions at each position. logits[t] = prediction for NEXT token after seeing 0..t. Correct alignment: logits[0] predicts x₁ (next after x₀) → compare with targets[1]. logits[1] predicts x₂ (next after x₁) → compare with targets[2]. logits[2] predicts x₃ (next after x₂) → compare with targets[3]. So we use: logits[:, :-1] (drop last, no next token to predict). targets[:, 1:] (drop first, it's the input not target). Concrete example: Input: 'The cat sat' = [23, 56, 89]. Forward produces logits.shape = (1, 3, vocab_size). logits[0, 0, :] = prediction after seeing 'The' → should match 'cat' (56). logits[0, 1, :] = prediction after seeing 'The cat' → should match 'sat' (89). logits[0, 2, :] = prediction after seeing 'The cat sat' → no target (end of sequence). Loss computation: loss = CE(logits[:, :-1, :].flatten(), targets[:, 1:].flatten()). Uses logits[0:2] against targets[1:3]. What happens with CrossEntropy(logits, targets) directly: Misalignment: logits[0] predicts next token, but targets[0] is the CURRENT token. Would compare: logits[0] (prediction after 'The') with targets[0] ('The'). This is wrong - model sees 'The', shouldn't predict 'The', should predict 'cat'! Result: Loss signal is meaningless - model trained on wrong task. Model would learn to predict current token (not useful). Training fails - no coherent learning objective. Why this matters: Autoregressive property is FUNDAMENTAL to language modeling. Shifting enforces: prediction is conditional on PAST only. This is what enables generation: given context, predict next. Breaking alignment breaks causality and the entire learning objective!",
    "required_concepts": [
      "Autoregressive: predict t+1 using 0..t",
      "logits[t] predicts next token after position t",
      "Alignment: logits[:, :-1] with targets[:, 1:]",
      "Drop last logit (no next token)",
      "Drop first target (it's input not target)",
      "Direct alignment wrong - breaks causality"
    ]
  },
  {
    "id": "transformer_lm_q2",
    "question": "Calculate memory usage during forward pass for GPT-2 small (batch=1, seq=512, d_model=768, layers=12, vocab=50257). Include embeddings, activations per layer, and logits. What dominates memory?",
    "model_answer": "Let's systematically calculate memory for single forward pass. Given: batch_size=1, seq_len=512, d_model=768, num_layers=12, num_heads=12, d_ff=3072, vocab_size=50257. Assume float32 (4 bytes per number). (1) Input and embeddings: Input IDs: (1, 512) int32 = 2KB (negligible). Token embeddings output: (1, 512, 768) = 1.57M floats = 6.3MB. (2) Per transformer layer activations: After RMSNorm1: (1, 512, 768) = 1.57M = 6.3MB. After attention: Q,K,V projections: 3 × (1, 512, 768) = 4.72M = 18.9MB. Reshape to heads: (1, 12, 512, 64) × 3 = same = 18.9MB. Attention scores: (1, 12, 512, 512) = 3.15M = 12.6MB. Attention output: (1, 12, 512, 64) = 1.57M = 6.3MB. After O proj: (1, 512, 768) = 1.57M = 6.3MB. After RMSNorm2: (1, 512, 768) = 1.57M = 6.3MB. FFN intermediate: W1,W3 outputs: 2 × (1, 512, 3072) = 6.29M = 25.2MB. After W2: (1, 512, 768) = 1.57M = 6.3MB. Total per layer: ~6.3 + 18.9 + 12.6 + 6.3 + 6.3 + 25.2 + 6.3 ≈ 82MB. (3) All 12 layers: Naive: 12 × 82MB = 984MB. But in reality: only need to store SOME activations for backprop. Minimum: input to each layer (for backprop) = 12 × 6.3MB = 75.6MB. Plus attention scores (needed for attention backprop) = 12 × 12.6MB = 151MB. Realistic estimate: ~200-300MB for layer activations. (4) Final outputs: After final RMSNorm: (1, 512, 768) = 6.3MB. Logits: (1, 512, 50257) = 26.4M floats = 105.6MB. (5) Gradients (during backprop): Must store gradients for all activations. Roughly doubles memory during training. Forward-only inference: ~400MB total. Training: ~800MB total. What dominates: Logits: 105.6MB (26% of total!). This is because vocab_size is huge (50K). Attention scores: 151MB across 12 layers (large due to seq_len²). FFN activations: Significant but less than above. Optimization insight: Logits are largest single tensor! Strategies to reduce: (1) Don't compute logits for all positions (only last for generation). (2) Chunk processing for long sequences. (3) Use float16 (halves memory). Attention scores O(seq²): For seq=2048: (1, 12, 2048, 2048) = 201M floats = 804MB just for attention in one layer! This is why long-context is memory-intensive. Flash Attention solves this by not materializing full attention matrix. Conclusion: For short sequences, logits dominate. For long sequences, attention scores dominate (quadratic growth). Memory grows with: batch×seq (linear) and seq² for attention (quadratic)!",
    "required_concepts": [
      "Logits: (batch, seq, vocab) = major memory consumer",
      "Attention scores: (batch, heads, seq, seq) = quadratic in seq",
      "Per-layer activations: ~80MB each layer",
      "Total: ~400MB forward, ~800MB training",
      "Logits dominate for short seq, attention for long seq",
      "vocab_size and seq_len² are memory bottlenecks"
    ]
  },
  {
    "id": "transformer_lm_q3",
    "question": "Tied embeddings (sharing weights between token_embeddings and lm_head) saves parameters. Derive the exact parameter savings for GPT-2 small (vocab=50257, d=768). Why doesn't this hurt performance? When would it hurt?",
    "model_answer": "Tied embeddings reuse input embedding matrix as output projection, saving significant parameters. Parameter count analysis: Without tying (separate weights): Token embeddings: vocab_size × d_model = 50257 × 768 = 38,597,376. LM head: vocab_size × d_model = 50257 × 768 = 38,597,376. Total: 77,194,752 parameters. With tying (shared weights): Shared weight matrix: vocab_size × d_model = 38,597,376. Total: 38,597,376 parameters. Savings: 38,597,376 parameters (50% reduction in embedding params!). For full GPT-2 small (124M total): Embeddings were 77M, now 38.6M. Total becomes: 124M - 38.6M = 85.4M parameters. Savings: 31% of full model! Why doesn't tying hurt performance: Mathematical insight: Input embedding: E[token_id] maps token → context vector. LM head: h @ W^T maps context → token logits. If we set W = E (tying), then: logit[v] = h @ E[v] = similarity between output context h and input embedding of token v. This is semantically reasonable! High logit = output context similar to target token's embedding. Empirical evidence: GPT-2, BERT, many models use tying with no performance loss. Some evidence it even HELPS (acts as regularization). The embedding space learns dual purpose: good input representations AND good output projections. Training dynamics: Backprop through both embedding lookup AND lm_head. Weight gets gradients from both: input context (what this token means given context). output prediction (what contexts predict this token). This dual signal is actually beneficial - more training signal per parameter. When would tying hurt: (1) Very different input/output distributions: If input tokens need different representation than output predictions. Example: Multi-task model (classify AND generate). Tying may be suboptimal. (2) Asymmetric vocab usage: Input: All tokens roughly equally common in context. Output: High-frequency tokens (the, a, is) dominate predictions. Separate weights could specialize. But empirically this doesn't matter much. (3) Different embedding vs. prediction needs: Input embeddings: Need to capture semantic similarity. Output logits: Need to be discriminative for classification. These objectives CAN conflict but usually align. (4) Non-standard architectures: If embedding dim ≠ model dim (d_embed < d_model for efficiency). Can't tie because shapes don't match. Models like ALBERT use this. Modern practice: Default: Use tying (GPT-2, GPT-3, many modern LLMs). Only untie if you have specific reason (multi-task, different dims). Savings justify tying unless proven otherwise. Conclusion: Tying saves 31% parameters for GPT-2 small with no performance cost. It works because input embeddings and output predictions share semantic space. Only untie for specific architectural reasons!",
    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "required_concepts": [
      "Tying saves 50% of embedding parameter    "is state_dict: state_dict: OrderedDict mapping parameter names to tensors. Created by model.state_dict(). Contains ALL learnable parameters with hierarchical names. Purpose: Save model weights to disk, load weights from checkpoint. Example state_dict structure: { 'token_embeddings.weight': Tensor(50257, 768), 'layers.0.ln1.weight': Tensor(768), 'layers.0.attn.q_proj.weight': Tensor(768, 768), 'layers.0.attn.k_proj.weight': Tensor(768, 768), ..., 'layers.11.ffn.w2.weight': Tensor(768, 3072), 'ln_final.weight': Tensor(768), 'lm_head.weight': Tensor(50257, 768) }. Why this format: Hierarchical naming: 'layers.{i}' groups parameters by layer number. 'attn.q_proj' identifies component within layer. Matches nn.Module structure: class TransformerLM(nn.Module): def __init__(self): self.layers = nn.ModuleList([ TransformerBlock(...) for _ in range(num_layers) ]). PyTorch automatically names: self.layers[0].attn.q_proj.weight → 'layers.0.attn.q_proj.weight'. Dot notation reflects object hierarchy. Standard convention: ALL PyTorch models follow this. Enables interoperability: save weights from any model, load into compatible architecture. Tooling (HuggingFace, PyTorch, etc.) expects this format. How we use it: To extract layer i weights: layer_i_keys = [k for k in weights.keys() if k.startswith(f'layers.{i}.')]. Specific parameter: q_weight = weights[f'layers.{i}.attn.q_proj.weight']. This requires EXACT key match. What breaks if we change format: Scenario: We use different keys like f'layer_{i}_q_proj'. (1) Loading fails: Trying to load checkpoint: model.load_state_dict(weights) → KeyError: 'layers.0.attn.q_proj.weight' not found. Model expects standard format, gets custom format. Crash! (2) Saving fails: Can't save with custom keys in standard checkpoint format. Other tools won't recognize structure. (3) Transfer learning breaks: Want to initialize from pretrained GPT-2. Pretrained weights have standard keys. Our model has custom keys. No way to map weights → can't use pretrained models! (4) Tooling incompatibility: HuggingFace transformers, PyTorch Lightning, etc. All assume standard format. Custom format breaks integration. (5) Debugging nightmare: Can't use standard model inspection tools. visualize_weights(model) fails. print(model.state_dict()) shows unexpected structure. Best practices: (1) Always use PyTorch's automatic naming (don't override). (2) Follow convention: layers/blocks, attn, ffn, etc. (3) Document any deviations clearly. (4) Test checkpoint save/load early. Why standardization matters: Deep learning: Extremely collaborative field. Researchers share pretrained weights constantly. Standard format enables: Model hubs (HuggingFace, PyTorch Hub). Quick prototyping (load GPT-2, fine-tune). Reproducibility (exact weight loading). Breaking convention: Isolates your model from ecosystem. Makes collaboration harder. Reduces code reuse. Conclusion: Key format f'layers.{i}.attn.q_proj.weight' is not arbitrary - it's PyTorch's state_dict standard. Matching this enables checkpointing, transfer learning, and tool compatibility. Change it and break the entire ecosystem!",
    "required_concepts": [
      "state_dict: OrderedDict of parameter names → tensors",
      "Hierarchical naming: 'layers.{i}.attn.q_proj.weight'",
      "Format matches nn.Module structure automatically",
      "Standard across PyTorch ecosystem",
      "Changing breaks: loading checkpoints, transfer learning, tooling",
      "Standardization enables collaboration and weight sharing"
    ]
  },
  {
    "id": "transformer_lm_q5",
    "question": "Compare parallel training (all positions simultaneously) vs sequential generation (one token at a time). For batch=32, seq=512, calculate the speedup. Why can't we generate in parallel? What is KV-caching and how does it help?",
    "model_answer": "Training exploits parallelism across positions, while generation must be sequential. Training (parallel): Input: Full sequence x = [x₀, x₁, ..., x₅₁₁] of length 512. Forward pass: One pass through model computes logits for ALL positions simultaneously. logits.shape = (32, 512, vocab_size). Loss: Compare all 512 predictions with targets in one step. Backprop: Single backward pass updates weights for all positions. Time: One forward + one backward pass regardless of sequence length. Compute: Fully parallel - use all GPU cores at once. Generation (sequential): Input: Prompt = [x₀, ..., x_p] of length p. Generate token 1: Forward pass with prompt → logits[:, -1, :]. Sample next token t₁. Append: [x₀, ..., x_p, t₁]. Generate token 2: Forward pass with [x₀, ..., x_p, t₁] → logits[:, -1, :]. Sample t₂. Append: [x₀, ..., x_p, t₁, t₂]. Repeat: N times for N new tokens. Time: N forward passes (one per new token). Compute: Sequential - can't parallelize across tokens. Speedup calculation: Training: 1 forward pass per batch. Generates predictions for 512 positions. Time per position: T_total / 512. Generation: 512 forward passes to generate 512 tokens. Time: 512 × T_forward. Speedup: Training is ~512× faster per position! For batch=32, seq=512: Training: Process 32 × 512 = 16,384 token predictions in 1 pass. Generation: Process 32 tokens (one per sequence) in 1 pass. Need 512 passes for 512 tokens → 16,384 predictions. Effective speedup: ~512× (sequence length factor). Why can't we generate in parallel: Autoregressive dependency: Token at position t depends on tokens 0..t-1. Can't compute position t+1 until we know what token t is. During training: We KNOW all tokens (they're in dataset). Can compute all positions given gold context. During generation: We DON'T know future tokens (haven't generated them yet!). Must generate one at a time. Causal constraint: Generation must respect causality. Each token influences all future tokens. Breaking this would violate the model's assumptions. What is KV-caching: Problem: Generating token t requires recomputing attention for positions 0..t-1. For position t, attention computes K, V for all previous tokens. But these were ALREADY computed when generating token t-1! Wasteful recomputation. KV-cache solution: During generation: Store K, V matrices for all past tokens. When generating token t+1: Compute K_new, V_new only for new token t. Concatenate with cached: K_full = concat(K_cached, K_new). V_full = concat(V_cached, V_new). Use K_full, V_full for attention. Cache K_full, V_full for next step. Savings: Without cache: Compute K,V for t tokens → O(t) computation per step. Total for N tokens: O(1+2+3+...+N) = O(N²). With cache: Compute K,V for 1 token → O(1) per step. Total for N tokens: O(N). Speedup: ~N× faster for N tokens! Implementation: cache = {'key': [], 'value': []}. For each new token: k_new = compute_key(new_token). v_new = compute_value(new_token). cache['key'].append(k_new). cache['value'].append(v_new). k_full = torch.cat(cache['key'], dim=seq_axis). v_full = torch.cat(cache['value'], dim=seq_axis). Use k_full, v_full for attention. Memory tradeoff: Cache size: (batch, heads, seq_len, d_head) per K and V. For seq=512, heads=12, d=64, batch=1: 2 × (1, 12, 512, 64) = 786K floats = 3.1MB per layer. 12 layers: 37.5MB total (manageable). Without cache: Recompute everything → more FLOPs. With cache: Store KV → more memory but fewer FLOPs. Modern practice: All production LLM inference uses KV-caching. Essential for interactive applications (chatbots, coding assistants). Trade memory for speed - worth it! Conclusion: Training is ~512× faster than generation (parallel vs sequential). Generation is sequential due to autoregressive dependencies. KV-caching provides ~N× speedup by avoiding recomputation. This is why generation feels slow compared to training throughput!",
    "required_concepts": [
      "Training: 1 pass for all 512 positions (parallel)",
      "Generation: 512 passes for 512 tokens (sequential)",
      "Speedup: ~512× (seq_len factor)",
      "Can't parallelize generation: autoregressive dependency",
      "KV-cache: Store past K,V, only compute new token",
      "Cache reduces O(N²) to O(N) for N tokens"
    ]
  }
]
