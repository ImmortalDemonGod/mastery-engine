[
  {
    "id": "text_generation_q1",
    "question": "Autoregressive generation samples one token at a time, conditioning on all previous tokens. Explain why we can't generate all tokens in parallel (like during training), how this affects inference speed (compare FLOPs for training vs generation), and why KV-caching is critical for production systems.",
    "model_answer": "Autoregressive generation is inherently sequential due to causal dependencies. Why can't we parallelize: (1) Causal dependency: Token t depends on tokens 1...(t-1). Must generate token t before token (t+1). Can't know future tokens to condition on! (2) Training vs inference difference: Training (parallel): Have full sequence [t1, t2, ..., tN]. Compute all predictions simultaneously with causal masking. Forward pass: single batch through model. Batch size B, sequence length N: O(B×N) throughput. Inference (sequential): Start with prompt [t1, ..., tk]. Generate t_{k+1} → Append → Generate t_{k+2} → ... Each token requires separate forward pass! Generate N tokens: N forward passes through model. FLOPs comparison: Training: Process 1000-token sequence: 1 forward pass. Cost: O(d²×1000) for model with dimension d. Inference: Generate 1000 tokens: 1000 forward passes! Cost: O(d²×1000×1000) = 1000× more expensive! Even worse: Each forward pass processes growing sequence. Token 1: process 10 tokens (prompt). Token 100: process 110 tokens (prompt + 100 generated). Token 500: process 510 tokens. Without optimization, this is O(N²) in sequence length! Why KV-caching is critical: Attention computation: For each token, compute Q×K^T×V. Keys and Values (K, V) depend on all previous tokens. Naive approach: Recompute K,V for ALL previous tokens every step. Token 100: Recompute K,V for tokens 1-99 (already computed before!). Massive waste! KV-caching optimization: Cache K,V from previous steps. Token 1: Compute K,V for prompt, cache them. Token 2: Only compute NEW K,V for token 2, append to cache. Use cached K,V for attention. Token 100: Only compute K,V for token 100, reuse cached K,V for 1-99. Savings: From O(N²) to O(N) in memory, from recomputing everything to computing once! Production impact: Without KV-cache: 100-token generation at 1 token/sec. With KV-cache: 100-token generation at 50 tokens/sec. 50× speedup! This is why ChatGPT, Claude can respond in real-time despite generating hundreds of tokens!",
    "required_concepts": [
      "Autoregressive: each token depends on previous tokens",
      "Can't parallelize due to causal dependencies",
      "Training: 1 forward pass, Inference: N forward passes",
      "Inference is ~1000× more FLOPs than training",
      "KV-caching: reuse Keys/Values from previous tokens",
      "KV-cache reduces O(N²) to O(N) computation"
    ]
  },
  {
    "id": "text_generation_q2",
    "question": "Temperature T scales logits before softmax: probs = softmax(logits/T). Derive mathematically why T→0 approaches greedy decoding (argmax) and T→∞ approaches uniform sampling. What happens at T=1.0?",
    "model_answer": "Temperature scaling changes the softmax sharpness. Let's prove the limit behavior. Temperature-scaled softmax: p_i = exp(logits_i / T) / Σ_j exp(logits_j / T). Case 1: T → 0 (approaching greedy). Let max_logit = max(logits). Rewrite: p_i = exp(logits_i / T) / Σ_j exp(logits_j / T). As T → 0: If logits_i = max_logit: exp(logits_i / T) → exp(max_logit / T) → very large. If logits_i < max_logit: logits_i / T → -∞ (since T→0, logits_i is fixed). exp(logits_i / T) → exp(-∞) → 0. Result: p_i → 1 if i = argmax(logits), p_i → 0 otherwise. This IS greedy decoding (argmax)! Example: logits = [2.0, 1.0]. T=0.1: exp(2.0/0.1) / (exp(2.0/0.1) + exp(1.0/0.1)) = exp(20) / (exp(20) + exp(10)) ≈ exp(20) / exp(20) = 0.9999. First token dominates! T→0: Only max token has probability. Case 2: T → ∞ (approaching uniform). Rewrite with T in denominator: p_i = exp(logits_i / T) / Σ_j exp(logits_j / T). As T → ∞: logits_i / T → 0 for all i. exp(logits_i / T) → exp(0) = 1 for all i. Result: p_i = 1 / Σ_j 1 = 1 / vocab_size. Uniform distribution! Example: logits = [10.0, 5.0, 1.0]. T=100: exp(10/100) / (exp(10/100) + exp(5/100) + exp(1/100)) = exp(0.1) / (exp(0.1) + exp(0.05) + exp(0.01)) ≈ 1.105 / (1.105 + 1.051 + 1.010) = 1.105 / 3.166 ≈ 0.35. Almost uniform (1/3 ≈ 0.33)! T→∞: All logits become equal after scaling. Case 3: T = 1.0 (normal). p_i = exp(logits_i) / Σ_j exp(logits_j). Standard softmax, no modification. Model's learned probabilities unchanged. Summary: T controls interpolation from greedy (T→0) to uniform (T→∞). T=1.0: Trust model's probabilities. T<1.0: More confident (sharper). T>1.0: Less confident (flatter). Practical values: T=0.7-0.9 for balanced creativity!",
    "required_concepts": [
      "Temperature scales logits: logits/T before softmax",
      "T→0: max logit dominates, others → 0 (greedy)",
      "T→∞: all logits → 0 after scaling (uniform)",
      "T=1.0: standard softmax (no change)",
      "T<1: sharpens distribution (more certain)",
      "T>1: flattens distribution (less certain)"
    ]
  },
  {
    "id": "text_generation_q3",
    "question": "Top-k sampling keeps only the k most probable tokens, while nucleus (top-p) sampling keeps the smallest set with cumulative probability ≥ p. Construct an example where top-k=10 is clearly suboptimal compared to top-p=0.9, showing how top-p adapts to model confidence.",
    "model_answer": "Top-p adapts to distribution shape, top-k doesn't. Here's a clear example: Scenario 1: Model is VERY CERTAIN (peaked distribution). Probabilities (sorted): [0.92, 0.03, 0.02, 0.01, 0.005, 0.005, 0.003, 0.002, 0.002, 0.001, ...]. Top-k=10: Keep all 10 tokens: [0.92, 0.03, 0.02, ..., 0.001]. Renormalize: [0.92/0.99, 0.03/0.99, ..., 0.001/0.99] ≈ [0.93, 0.03, ...]. Problem: Tokens 5-10 have probability ~0.001 (basically noise!). Model is 92% sure about token 1, but we're including garbage tokens. Top-p=0.9: Cumulative: [0.92, 0.95, 0.97, ...]. Keep until cumsum ≥ 0.9: Only token 1 (cumsum = 0.92 ≥ 0.9). Sample from: Just token 1 (probability ~1.0 after renormalization). Result: Top-p correctly identifies model is certain, focuses on best token. Top-k wastes probability on 9 unlikely tokens. Scenario 2: Model is VERY UNCERTAIN (flat distribution). Probabilities: [0.08, 0.08, 0.07, 0.07, 0.06, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04, 0.04, 0.03, ...]. Top-k=10: Keep first 10 tokens: [0.08, 0.08, ..., 0.04]. Cumulative of top 10: 0.08+0.08+...+0.04 = 0.61. Problem: Only 61% of probability mass! Tokens 11-13 (probabilities 0.04, 0.03) are nearly as likely as token 10. Arbitrary cutoff excludes reasonable options. Top-p=0.9: Cumulative: [0.08, 0.16, 0.23, 0.30, ..., 0.85, 0.90, ...]. Keep until cumsum ≥ 0.9: ~15-20 tokens. Sample from: All reasonable tokens with meaningful probability. Result: Top-p correctly identifies model is uncertain, includes more options. Top-k arbitrarily cuts off at 10, missing valid tokens. Why top-p is superior: Adapts to model confidence: Certain distribution → small set. Uncertain distribution → large set. Top-k is fixed: Always exactly k tokens, regardless of distribution shape. Can include noise (peaked case) or exclude reasonable options (flat case). Real example (ChatGPT-like model): Prompt: 'The capital of France is' → Model 99.9% sure: 'Paris'. Top-k=10: Includes 'London' (0.001%), 'Berlin' (0.0005%) - garbage! Top-p=0.9: Only 'Paris' - correct! Prompt: 'I feel very' → Model uncertain: many emotions possible. Top-k=10: Cuts off at 10 emotions arbitrarily. Top-p=0.9: Includes all 20+ reasonable emotions. This is why modern LLMs (GPT-4, Claude) use top-p, not top-k!",
    "required_concepts": [
      "Top-k: fixed k tokens always",
      "Top-p: variable set size, adapts to distribution",
      "Peaked distribution: top-p uses fewer tokens (correct)",
      "Flat distribution: top-p uses more tokens (correct)",
      "Top-k includes noise in peaked case",
      "Top-k excludes valid options in flat case"
    ]
  },
  {
    "id": "text_generation_q4",
    "question": "During generation, we must use model.eval() and torch.no_grad(). Explain exactly what goes wrong if you forget each: (1) What if you use model.train() mode during generation? (2) What if you forget torch.no_grad()? Give concrete examples of failures.",
    "model_answer": "Both settings are critical for correct and efficient inference. Forgetting each causes different failures. (1) Using model.train() mode (forgot model.eval()): What train() mode does: Dropout active: Randomly zeros activations with probability p. BatchNorm updates: Uses batch statistics, updates running mean/std. Both are stochastic and inappropriate for inference! Concrete failure example: Model has dropout(p=0.1) and generates text. Generation attempt 1: 'The quick brown fox jumps'. 10% of activations randomly dropped → some words affected. Generation attempt 2 (same prompt): 'The quick brown leopard leaps'. Different 10% dropped → different words! Generation attempt 3: 'The fast auburn fox bounds'. Yet different dropout → yet different output! Problem: Non-deterministic generation even with greedy decoding! Same prompt should give same output (with greedy/low temp). Dropout makes it random → unreliable. BatchNorm failure: Single-sample inference: Batch size = 1. BatchNorm tries to compute mean/std over 1 sample → meaningless! Running stats not used, batch stats are just that one sample. Normalization becomes identity or breaks. Result: Model behavior completely wrong, degraded quality. (2) Forgetting torch.no_grad() (kept gradient tracking): What no_grad() prevents: PyTorch tracks operations for autograd. Stores intermediate activations for backward pass. Builds computation graph. None of this is needed during inference! Concrete failure example: Generate 100 tokens. Without no_grad(): Token 1: Forward pass, stores activations A1 in memory. Token 2: Forward pass, stores A2 (A1 still in memory!). Token 3: Forward pass, stores A3 (A1, A2 still in memory!). ... Token 100: A1, A2, ..., A100 ALL stored! Memory usage: 100× baseline (every token's activations kept). GPU OOM (out of memory) after ~50 tokens! With no_grad(): Token 1: Forward pass, immediately discards activations. Token 2: Forward pass, discards (only one set of activations at a time). Memory stays constant! Performance impact: Gradient tracking overhead: ~30% slower forward pass. Memory: 10-100× more memory used. OOM crashes: Can't generate long sequences. Real example: LLaMA-7B generating 500 tokens. Without no_grad(): OOM at 50 tokens on 24GB GPU. With no_grad(): Generates 500 tokens smoothly, uses 8GB. Correct usage: model.eval() # Deterministic behavior (no dropout/batchnorm randomness) with torch.no_grad(): # No gradient tracking (memory efficient) output = generate(model, prompt) Both are essential for production inference!",
    "required_concepts": [
      "model.train(): dropout active, batchnorm updates stats",
      "Forgot eval(): non-deterministic generation, wrong batchnorm",
      "torch.no_grad(): prevents gradient tracking",
      "Forgot no_grad(): stores all activations, 100× memory",
      "OOM crashes without no_grad()",
      "Both eval() and no_grad() required for inference"
    ]
  },
  {
    "id": "text_generation_q5",
    "question": "Modern systems combine temperature and nucleus sampling: probs = softmax(logits/temperature), then apply top-p. Explain why this combination is better than either alone, and recommend specific (temperature, top-p) pairs for: (1) creative writing, (2) code generation, (3) factual QA.",
    "model_answer": "Temperature and top-p serve complementary roles and combine multiplicatively. Why combine both: Temperature adjusts 'how confident' the distribution is: Scales logits: logits/T. Changes relative probabilities (sharpens or flattens). Affects ALL tokens uniformly. Top-p selects 'which tokens to consider': Filters out low-probability tail. Adapts to distribution shape. Different sets for different predictions. Together: Temperature shapes the distribution. Top-p filters it. This gives fine-grained control! Example: Original logits: [3.0, 2.0, 1.0, 0.5, 0.1, 0.1, ...]. Step 1 - Apply temperature T=0.8: logits/T: [3.75, 2.5, 1.25, 0.625, 0.125, 0.125, ...]. probs: [0.65, 0.22, 0.08, 0.03, 0.01, 0.01, ...] (sharper!). Step 2 - Apply top-p=0.9: Cumsum: [0.65, 0.87, 0.95, ...]. Keep tokens 1-3 (cumsum=0.95 ≥ 0.9). Renormalize: [0.65/0.95, 0.22/0.95, 0.08/0.95] = [0.68, 0.23, 0.09]. Final: High-quality tokens with adjusted confidences! Why better than each alone: Temperature alone (T=0.8, no top-p): Sharpens distribution but doesn't remove tail. Still can sample token 6 (prob 0.01) occasionally. Low-prob tokens cause rare but bad outputs. Top-p alone (p=0.9, T=1.0): Filters tail but doesn't adjust confidence. If model is overconfident (prob 0.99 on one token), top-p doesn't help. If model is underconfident (flat distribution), top-p keeps too many tokens. Combination: Temperature adjusts confidence to desired level. Top-p removes inappropriate tail. Best of both worlds! Recommended settings: (1) Creative writing (stories, poetry): Temperature: 0.9-1.1 (higher creativity). Top-p: 0.9-0.95 (allow diverse word choices). Why: Want surprising, creative outputs. Model can explore unusual phrases. Avoid repetitive, boring text. Example: 'The moon cast a silver glow' instead of 'The moon was bright'. (2) Code generation: Temperature: 0.2-0.5 (lower creativity). Top-p: 0.9 (still filter tail). Why: Code must be syntactically correct. Low temp ensures high-confidence tokens (likely correct syntax). Top-p still helps remove nonsense tokens. Example: Generates 'def function():' not 'dfe functoin(:' (typos from low-prob tokens). (3) Factual QA: Temperature: 0.1-0.3 (very low creativity). Top-p: 0.9 (standard filtering). Why: Facts have one correct answer. Want model's most confident prediction. Near-greedy behavior. Example: 'What is 2+2?' → '4' (not '3.9' or '4.1' from sampling). Industry standards: ChatGPT: T≈0.7, p=0.9 (balanced). Claude: T≈0.7, p=0.9 (balanced). GitHub Copilot (code): T≈0.2, p=0.9 (precise). Creative AI (stories): T≈1.0-1.2, p=0.95 (creative). The combination is now universal in production systems!",
    "required_concepts": [
      "Temperature shapes distribution (sharpens/flattens)",
      "Top-p filters distribution (removes tail)",
      "Combined: shape then filter (complementary)",
      "Creative writing: high T (0.9-1.1), high p (0.9-0.95)",
      "Code generation: low T (0.2-0.5), standard p (0.9)",
      "Factual QA: very low T (0.1-0.3), standard p (0.9)"
    ]
  }
]
