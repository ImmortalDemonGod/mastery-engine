[
  {
    "id": "linear_q1",
    "question": "The Linear layer computes y = x @ W^T + b where W has shape (out_features, in_features). Explain why we transpose W in the forward pass instead of storing it as (in_features, out_features). What are the implications for memory layout and cache efficiency?",
    "model_answer": "The transpose exists due to PyTorch's weight storage convention and has performance implications. Why store as (out, in) and transpose: (1) Semantic intuition: Each row of W represents one output neuron's weights. Row 0 = weights for output[0], row 1 = weights for output[1], etc. This matches how we think about neurons. (2) Dimension matching: Forward pass needs (batch, in) @ (in, out) = (batch, out). If W is (out, in), then W^T is (in, out) which matches! (3) Backward pass efficiency: ∂L/∂W = (∂L/∂y)^T @ x computes outer product naturally fitting (out, in) shape. Alternative (storing as in×out): Could store W as (in, out) and skip transpose. Then forward is x @ W directly. But this breaks: (a) Semantic clarity - rows wouldn't represent output neurons. (b) Backward pass becomes awkward - would need transpose there instead. (c) PyTorch convention - all frameworks use (out, in) for consistency. Memory and cache implications: Transpose operation: Doesn't copy data, just changes stride metadata. W.t() is O(1), not O(n²)! Creates a view with different memory access pattern. Cache efficiency: W stored row-major (C-contiguous). Each row = one output neuron's weights, stored contiguously. During matmul, we access W^T column-major (reading across rows of original W). For small matrices: Cache-friendly, entire W fits in cache. For huge matrices (out=50K vocab, in=4096 model): Accessing W^T column-wise can cause cache misses. Modern optimization: cuBLAS/MKL implement optimized transpose-multiply kernels. Hardware prefetching mitigates cache issues. Overall: Convention wins - (out, in) storage is standard across PyTorch, TensorFlow, Jax for semantic clarity and ecosystem compatibility.",
    "required_concepts": [
      "W shape (out, in) matches semantic intuition of output neurons",
      "Transpose needed for dimension matching in matmul",
      "Transpose is O(1) view operation, not copy",
      "Row-major storage means rows are contiguous",
      "Column-wise access of W^T can cause cache misses",
      "Convention chosen for clarity and ecosystem consistency"
    ]
  },
  {
    "id": "linear_q2",
    "question": "Weight initialization uses uniform(-1/√fan_in, 1/√fan_in). Derive why this specific range keeps activation variance stable. What happens with N(0,1) initialization instead? Why does this matter for deep networks?",
    "model_answer": "The 1/√fan_in bound is derived from variance preservation requirements. Derivation for variance stability: Assume input x with mean 0, variance σ_x². Linear layer: y = Wx + b. We want Var(y) ≈ Var(x) to prevent explosion/vanishing. Consider one output: y_i = Σⱼ W_ij x_j. Variance: Var(y_i) = Σⱼ Var(W_ij x_j) = Σⱼ Var(W_ij) Var(x_j) (assuming independence). If W_ij ~ U(-a, a): Var(W_ij) = a²/3. With fan_in inputs: Var(y_i) = fan_in · (a²/3) · σ_x². For variance preservation (Var(y) = Var(x)): fan_in · (a²/3) · σ_x² = σ_x². Solving: a² = 3/fan_in, so a = √(3/fan_in) ≈ 1/√fan_in (ignoring constant). This is Kaiming/He initialization for linear activations! What happens with N(0,1) initialization: W_ij ~ N(0, 1) means Var(W) = 1. Then Var(y_i) = fan_in · 1 · σ_x² = fan_in · σ_x². Variance EXPLODES by factor of fan_in! Example: fan_in=768, after one layer variance is 768× larger. After 12 layers: variance is 768¹² ≈ 10³⁵ (complete explosion!). Activations become huge, gradients overflow, training fails. Why this matters for deep networks: Deep network (12+ layers): Bad init propagates: Layer 1 output variance 768×. Layer 2 sees huge inputs, outputs 768² ×. Layer 12: variance is astronomical. Result: Forward pass: activations explode to inf or vanish to 0. Backward pass: gradients explode (NaN) or vanish (stuck). Training: Doesn't converge, loss becomes NaN within a few steps. Good initialization (1/√fan_in): Each layer preserves variance. After 12 layers: variance stays approximately constant. Training: Stable gradients, successful optimization. Historical impact: Pre-2010: Deep networks (>5 layers) were nearly impossible to train. Xavier/Kaiming init (2010s): Unlocked training of very deep networks. Modern era: 100+ layer networks train successfully with proper initialization. Proper initialization is prerequisite for deep learning to work!",
    "required_concepts": [
      "Variance of sum: Var(Σ W_ij x_j) = Σ Var(W) Var(x)",
      "For stability: want Var(output) = Var(input)",
      "Uniform(-a, a): Var = a²/3",
      "Solving gives a = √(1/fan_in) for variance preservation",
      "N(0,1) init causes variance explosion by factor fan_in per layer",
      "Deep networks: bad init causes exponential explosion/vanishing"
    ]
  },
  {
    "id": "linear_q3",
    "question": "Modern Transformers omit bias (bias=False) in most linear layers. Explain why bias is often unnecessary when normalization layers (RMSNorm, LayerNorm) are present. Under what conditions is bias still beneficial?",
    "model_answer": "Bias becomes redundant when paired with normalization layers due to centering operations. Why normalization makes bias redundant: LayerNorm/RMSNorm compute: normalized = (x - mean) / std. The subtraction of mean ELIMINATES any constant bias! Example: y = Wx + b (with bias b). Then LayerNorm(y) = (Wx + b - mean(Wx + b)) / std. The constant b shifts the mean: mean(Wx + b) = mean(Wx) + b. So (Wx + b - mean(Wx + b)) = (Wx + b - mean(Wx) - b) = Wx - mean(Wx). The bias b cancels out! RMSNorm is similar but doesn't subtract mean, yet still normalizes magnitude. Concrete example: Linear: y = Wx + b = [5.0, 3.0, 7.0] + [2.0, 2.0, 2.0] = [7.0, 5.0, 9.0]. LayerNorm: mean=7.0, after centering: [0, -2, 2] (bias cancelled). Then divide by std and scale by learned weight. Result: The learned bias b had no effect after normalization! Benefits of omitting bias: Parameter reduction: For d_model=4096, each linear layer saves 4K parameters. In a 12-layer model with 4 linears per layer: 48 × 4K = 196K parameters saved. Training stability: Fewer parameters = less overfitting. Normalization handles centering, bias is redundant degree of freedom. Simplicity: Code is simpler, one less thing to initialize/update. When bias IS still beneficial: (1) Output layers without normalization: LM head projection (no norm after): Linear(d_model, vocab_size, bias=True). Bias allows shifting logits (e.g., modeling word frequency). (2) First layer (input embeddings): Embedding + positional encoding might need centering. Though modern transformers still use bias=False here. (3) Legacy architectures: Older models (pre-2020) used bias everywhere by default. Modern trend: GPT-2: Uses bias=True in most layers (2019 design). LLaMA, GPT-3, modern models: bias=False throughout (2020+ design). Saves millions of parameters with no performance loss! The shift to bias=False is a modern best practice enabled by understanding the interaction between linear layers and normalization. It's not just about parameter count - it's about recognizing which parameters are truly necessary!",
    "required_concepts": [
      "LayerNorm subtracts mean, cancelling constant bias",
      "Normalization mean-centers: (x + b - mean(x + b)) = x - mean(x)",
      "Bias becomes redundant parameter after normalization",
      "Omitting bias saves parameters (4K per layer for d=4096)",
      "Bias still useful in output layers without normalization",
      "Modern Transformers use bias=False as best practice"
    ]
  },
  {
    "id": "linear_q4",
    "question": "A 12-layer Transformer with d_model=768, d_ff=3072 uses 3 linear layers per FFN. Calculate: (1) FFN parameters per layer, (2) attention projection parameters per layer (Q,K,V,O), (3) total linear layer parameters. Which dominates?",
    "model_answer": "Let's calculate systematically for a GPT-2 small-sized model. (1) FFN parameters per layer: SwiGLU uses 3 linear layers: W1: (d_ff, d_model) = (3072, 768) = 2,359,296 parameters. W2: (d_model, d_ff) = (768, 3072) = 2,359,296 parameters. W3: (d_ff, d_model) = (3072, 768) = 2,359,296 parameters. Total FFN per layer = 7,077,888 ≈ 7.08M parameters. (2) Attention projection parameters per layer: Multi-head attention uses 4 projections: Q: (d_model, d_model) = (768, 768) = 589,824 parameters. K: (d_model, d_model) = (768, 768) = 589,824 parameters. V: (d_model, d_model) = (768, 768) = 589,824 parameters. O: (d_model, d_model) = (768, 768) = 589,824 parameters. Total attention per layer = 2,359,296 ≈ 2.36M parameters. (3) Total linear parameters for 12-layer model: Per layer: FFN (7.08M) + Attention (2.36M) = 9.44M. For 12 layers: 12 × 9.44M = 113.28M parameters. Breakdown: FFN total: 12 × 7.08M = 84.96M (75% of linear params). Attention total: 12 × 2.36M = 28.32M (25% of linear params). Which dominates: FFN dominates by 3:1 ratio! Why: d_ff = 4 × d_model (standard ratio). FFN has 3 projections involving d_ff (3072). Attention has 4 projections but all d_model × d_model (768²). Calculation: FFN = 3 × d_model × d_ff = 3 × 768 × 3072 ≈ 7.08M. Attention = 4 × d_model² = 4 × 768² ≈ 2.36M. Ratio: FFN/Attention = (3 × d_model × 4×d_model) / (4 × d_model²) = 3×4/4 = 3. So FFN is always 3× larger (with standard d_ff=4×d_model)! Comparison to full model: GPT-2 small total: 124M parameters. Linear layers: 113M (91% of model!). Embeddings: ~50K vocab × 768 = 38M. Normalization: negligible (12 × 2 × 768 = 18K). Other: position embeddings, small components. Conclusion: Linear layers dominate parameter count in Transformers. FFN layers are the largest component (75% of linear params, 68% of model). Attention is significant but smaller (25% of linear params, 23% of model). This is why FFN optimization (e.g., MoE, sparse FFN) has huge impact!",
    "required_concepts": [
      "FFN: 3 layers × d_model × d_ff = 7.08M per layer",
      "Attention: 4 projections × d_model² = 2.36M per layer",
      "FFN dominates 3:1 over attention (with d_ff=4×d_model)",
      "12 layers: 113M total linear parameters",
      "Linear layers are 91% of GPT-2 small parameters",
      "FFN is largest single component at 68% of model"
    ]
  },
  {
    "id": "linear_q5",
    "question": "During forward pass, we compute y = x.matmul(W.t()) where W.t() creates a transposed view. Explain how PyTorch's tensor views work (no data copy), what happens during backward pass, and why this design enables memory-efficient automatic differentiation.",
    "model_answer": "PyTorch's view system and autograd interact elegantly for memory efficiency. How tensor views work (no copy): W shape (out, in), stored row-major with stride (in, 1). W[i,j] at memory location: base + i×in + j. W.t() creates transposed view: Same underlying data pointer. Swaps strides: W.t() has stride (1, in) instead of (in, 1). W.t()[i,j] accesses same memory as W[j,i]. No data copied! View created in O(1) time, zero memory cost. Example: W = [[1,2,3], [4,5,6]] stored as [1,2,3,4,5,6]. W shape (2,3), stride (3,1). W.t() shape (3,2), stride (1,3), SAME data [1,2,3,4,5,6]. W.t()[0,1] = memory[0 + 1×3] = memory[3] = 4 = W[1,0] ✓. Forward pass implications: x.matmul(W.t()) must handle transposed stride. cuBLAS/MKL detect stride pattern and use optimized transpose-gemm. No explicit transpose copy needed in fast path. Backward pass mechanics: Forward: y = x @ W^T. Autograd records: operation, inputs (x, W.t() view). Backward: Given ∂L/∂y, compute ∂L/∂x and ∂L/∂W. ∂L/∂x = (∂L/∂y) @ W. (uses W directly, not W.t()!). ∂L/∂W = (∂L/∂y)^T @ x. Key insight: Gradient flows back through view! ∂L/∂(W.t()) is computed. PyTorch automatically accumulates to ∂L/∂W. The view just changes interpretation, not data. W.grad receives accumulated gradient directly in (out, in) storage. Memory efficiency of this design: Single storage: W and W.t() share memory. Only one tensor in memory, multiple views. Backward pass: No need to store transposed copy for backprop. Autograd traces through view operation. Gradient computed and accumulated to original W. Memory saved: For W (50000, 4096) = 200M floats = 800MB. Without views: would need 800MB for W + 800MB for W^T = 1.6GB. With views: only 800MB total. 2× memory savings! This compounds: Model with 100 linear layers and various views/reshapes. Without view system: could easily use 10× more memory. With views: minimal memory overhead for operations. Why this enables automatic differentiation: View operations recorded in computation graph. Backward pass follows graph, applying chain rule through views. Views are cheap (O(1) time/space), so backprop through views is free. This allows flexible tensor manipulation without memory penalty. Result: PyTorch enables complex operations (transpose, reshape, slice, etc.) with zero memory cost, while autograd seamlessly handles differentiation through these operations. This is a core design decision that makes modern deep learning practical!",
    "required_concepts": [
      "W.t() creates view with swapped strides, no data copy",
      "View is O(1) operation, zero memory overhead",
      "Forward: cuBLAS handles transposed stride efficiently",
      "Backward: gradient flows through view to original tensor",
      "View system saves memory (2× for transposed weights)",
      "Autograd seamlessly differentiates through views"
    ]
  }
]
