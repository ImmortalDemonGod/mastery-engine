{
  "questions": [
    {
      "id": "q1_deque_complexity",
      "question": "Why does using `list.pop(0)` in BFS create O(n²) time complexity while `deque.popleft()` maintains O(n)? Explain the memory layout difference.",
      "model_answer": "When you call list.pop(0), Python must shift all remaining elements one position to the left to fill the gap at index 0. For n elements, this is O(n) per pop operation. In BFS, you pop n times, giving O(n²) total. A deque uses a doubly-linked list or circular buffer internally, where removing from either end only requires updating pointers/indices (O(1)). The head pointer moves forward without touching other elements. For BFS on a graph with 10,000 nodes, list.pop(0) does ~50 million operations vs. deque.popleft() doing ~10,000.",
      "failure_modes": [
        "both are O(1)",
        "no real difference",
        "just a style choice",
        "lists are faster"
      ],
      "required_concepts": [
        "array shifting",
        "linked list pointers",
        "amortized complexity",
        "memory layout"
      ]
    },
    {
      "id": "q2_heapq_vs_sorting",
      "question": "In Dijkstra's algorithm, why is using `heapq` (O(E log V)) better than repeatedly sorting a list (O(E·V log V))? Provide a concrete example with numbers.",
      "model_answer": "Consider a graph with V=1000 nodes and E=5000 edges. With heapq: each edge relaxation pushes to heap (O(log V) = ~10 ops), total ~50,000 operations. With sorting: each iteration sorts all remaining nodes (O(V log V) = ~10,000 ops), done E times = ~50 million operations. The heap maintains partial order (only the minimum is guaranteed at top), while sorting maintains full order (wasteful when you only need the minimum). Heapq extracts the minimum in O(log V) without reordering the entire structure, whereas sorting reorders all V elements every time.",
      "failure_modes": [
        "sorting is simpler",
        "no practical difference",
        "both are log n",
        "sorting is O(n log n)"
      ],
      "required_concepts": [
        "heap property",
        "partial vs full ordering",
        "big-O notation",
        "edge relaxation"
      ]
    },
    {
      "id": "q3_bisect_correctness",
      "question": "When counting elements in range [low, high], why must you use `bisect_left(low)` and `bisect_right(high)` specifically? What goes wrong if you use bisect_right for both?",
      "model_answer": "bisect_left(low) returns the first index where arr[i] >= low (leftmost position). bisect_right(high) returns the first index where arr[i] > high (one past the rightmost position). For array [1,3,3,3,5] and range [3,3], bisect_left(3)=1 and bisect_right(3)=4, so count = 4-1 = 3 (correct). If you use bisect_right(low)=4 and bisect_right(high)=4, count = 4-4 = 0 (wrong). The left boundary must be inclusive (>=) and the right exclusive (>) to correctly capture [low, high]. Using the same function for both boundaries fails when low == high or when the array contains duplicates at the boundaries.",
      "failure_modes": [
        "they're the same thing",
        "doesn't matter which",
        "just pick one",
        "both return the same index"
      ],
      "required_concepts": [
        "inclusive vs exclusive bounds",
        "binary search variants",
        "duplicate handling",
        "half-open intervals"
      ]
    }
  ]
}
