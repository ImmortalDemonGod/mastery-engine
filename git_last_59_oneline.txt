6ad4b45 ğŸ“ docs(worklog): update worklog with diagnostic probe adaptations
6bd4279 âœ¨ feat(scripts): add one-dimensional probing model
b205b34 ğŸ“ docs(WORKLOG): update work log with recent progress
ce8a339 âœ¨ feat(bpe): add byte pair encoding implementation
0e2c6c1 âœ¨ feat(tokenizer): add byte-level BPE tokenizer
97a2210 âœ¨ feat(tests): implement checkpoint and tokenizer functions
ad8b887 ğŸ“ docs(worklog): update worklog with transformer LM and block implementation details
9d1da21 âœ¨ feat(layers): add transformer language model implementation
249b8aa âœ¨ feat(tests): implement transformer language model runner
12316a7 âœ… test(adapters): add transformer block implementation test
0b22028 âœ¨ feat(layers): add transformer block with RoPE functionality
f8061f2 ğŸ“ docs(worklog): document implementation of Multi-Head Self-Attention
f1f5600 âœ¨ feat(layers): add Rotary Positional Embedding (RoPE)
e11c54e âœ¨ feat(tests): implement multihead attention with RoPE
01c9edb ğŸ“ docs(worklog): update worklog entries with recent changes
49af06d âœ¨ feat(layers): add multihead self-attention implementation
c168faf âœ¨ feat(tests): implement multihead self-attention function
34748c9 ğŸ“ docs(worklog): update worklog entries for attention implementation
b3990b5 ğŸ“ docs(worklog): add work log entry for scaled dot-product attention implementation
1f59b39 âœ¨ feat(tests): implement scaled dot product attention
b1f1658 âœ¨ feat(layers): add scaled dot-product attention function
b6161da ğŸ“ docs(worklog): document recent implementations and tests
af1aaf6 âœ¨ feat(layers): add SwiGLU feed-forward block
672abea âœ¨ feat(tests): add SwiGLU layer to adapters
9cc8e87 âœ¨ feat(tests): implement RMSNorm and SiLU functions
60c5e4d ğŸ“ docs(worklog): update worklog with embedding and linear module details
6be2b35 âœ¨ feat(layers): add embedding and normalization layers
114c7b5 âœ¨ feat(tests): implement embedding and RMSNorm layers
3d9fa41 âœ¨ feat(layers): implement Linear layer from scratch
5b3bb23 ğŸ“ docs(worklog): add entry for checkpoint utility functions
d142a69 ğŸ› fix(tests): remove not implemented error from tests
6aa688c ğŸ”§ chore(tests): update placeholder implementations in adapters
71c9974 ğŸ› fix(tests): remove unimplemented function calls
cdf47f7 âœ¨ feat(tests): add checkpoint saving and loading functions
9aa612e âœ¨ feat(utils): add model checkpointing functionality
745ad71 ğŸ“ docs(worklog): update worklog with recent implementation details
ee5ae4f âœ¨ feat(tests): implement run_get_batch function
ead0eed âœ¨ feat(utils): add function to get LM batches from dataset
7000172 âœ¨ feat(tests): implement batch retrieval in run_embedding
61fba0f ğŸ“ docs(worklog): add entry for implementing AdamW optimizer
6e08726 âœ¨ feat(tests): implement AdamW optimizer in tests
8f56876 âœ¨ feat(optimizer): add AdamW optimizer implementation
bcbe357 ğŸ“ docs(worklog): add worklog entry for get_lr_cosine_schedule
49dad89 âœ¨ feat(tests): implement learning rate cosine schedule function
1d6a826 âœ¨ feat(utils): add cosine learning rate schedule function
0a66252 ğŸ“ docs(worklog): add worklog entry for gradient clipping implementation
6e91baa âœ¨ feat(tests): implement gradient clipping in run_gradient_clipping
ea7c029 âœ¨ feat(utils): add gradient clipping function
313baba ğŸ“ docs(worklog): update worklog with cross_entropy implementation details
d7a6ba1 ğŸ“ docs(worklog): update worklog entries for softmax implementation
6cc155f ğŸ› fix(tests): correct implementation of run_linear and run_cross_entropy
91277c4 âœ¨ feat(tests): implement linear and embedding run functions
2d198f3 âœ¨ feat(tests): add cross entropy implementation in tests
263f658 âœ¨ feat(utils): add cross-entropy loss function
db12f18 â™»ï¸ refactor(utils): improve softmax implementation
bc3cc2d ğŸ› fix(tests): implement softmax in run_softmax function
2331d23 âœ¨ feat(tests): implement softmax in run_linear function
96cdbdc âœ¨ feat(utils): add numerically-stable softmax function
bedbf1f ğŸ“ docs: add implementation and repository analysis documents
