# LLM Prompt Structure Review

## Current Implementation

The LLM receives the following prompt structure for evaluating justify answers:

### System Prompt
```
You are an expert technical educator evaluating student understanding. 
Analyze the student's answer using chain-of-thought reasoning, then provide 
a structured evaluation in JSON format.
```

### User Prompt Template

```
You are evaluating a student's answer to a technical question. Use chain-of-thought reasoning to determine if they demonstrate sufficient understanding.

**Question:**
{question.question}

**Student's Answer:**
{user_answer}

**Model Answer (for reference):**
{question.model_answer}

**Required Concepts:**
The student's answer must demonstrate understanding of these concepts:
- {concept_1}
- {concept_2}
- {concept_3}
- ...

**Evaluation Instructions:**
1. **Identify Concepts**: What key concepts did the student mention in their answer?
2. **Compare to Required**: Do they demonstrate understanding of the required concepts?
3. **Check for Errors**: Are there any conceptual errors or misconceptions?
4. **Decision**: Based on the above, is the answer correct (demonstrates sufficient understanding)?
5. **Feedback**: 
   - If CORRECT: Write a brief acceptance message acknowledging their understanding
   - If INCORRECT: Write a Socratic hint that guides them toward the missing concept WITHOUT giving away the answer

**Output Format:**
You MUST respond with valid JSON in this exact format:
{
    "is_correct": true or false,
    "feedback": "Your feedback message here"
}

Think step-by-step, then provide your final evaluation in JSON format.
```

## Example: Softmax Question

### Actual Prompt Sent for Softmax

**Question:**
```
Justify the subtract-max trick in your softmax implementation. Why is softmax(x - max(x)) 
mathematically equivalent to softmax(x), and what specific numerical problem does this solve?
```

**Model Answer:**
```
The subtract-max trick exploits the mathematical identity: softmax(x) = softmax(x - c) 
for any constant c, because both numerator and denominator are multiplied by exp(-c), 
which cancels out. By choosing c = max(x), we shift the input range to (-inf, 0], 
ensuring the largest exponent is exp(0) = 1.0. This prevents overflow (since exp(large 
positive) ‚Üí inf) while gracefully handling underflow (since exp(large negative) ‚Üí 0). 
The final normalization produces identical results to the naive implementation, but with 
numerical stability.
```

**Required Concepts:**
- mathematical equivalence
- exp(c) cancellation
- overflow prevention
- range shift to (-inf, 0]

## Assessment of Context Sufficiency

### ‚úÖ What's Provided
- **Full question text** - LLM knows exactly what was asked
- **Student's complete answer** - Full context of their response
- **Model answer** - Reference standard for comparison
- **Explicit rubric** - Required concepts listed as bullets
- **Step-by-step instructions** - Clear evaluation framework
- **Output format** - Structured JSON schema

### ‚ö†Ô∏è Potential Gaps
1. **No module context** - LLM doesn't know this is about "Numerically Stable Softmax" module
2. **No implementation context** - No reference to what the student actually implemented
3. **No previous interaction history** - Each evaluation is stateless
4. **No difficulty calibration** - LLM doesn't know if this is beginner/advanced level

### üîç Recommendations

#### High Priority
1. **Add module context** to system prompt:
   ```
   You are evaluating understanding of: {module.name}
   Context: {brief_module_description}
   ```

2. **Include implementation reference**:
   ```
   **Student's Implementation:**
   The student successfully implemented this function: {function_signature}
   Tests confirmed: {what_passed}
   ```

#### Medium Priority
3. **Add curriculum context**:
   ```
   Curriculum: CS336 - Deep Learning Systems
   Expected knowledge level: Graduate-level ML engineering
   ```

4. **Provide failure mode hints**:
   ```
   Common misconceptions for this question:
   - {failure_mode_1.category}: {brief_description}
   - {failure_mode_2.category}: {brief_description}
   ```

#### Low Priority
5. **Add conversation context** (if needed):
   - Track if this is a retry
   - Reference previous feedback given

## Testing the Current Prompt

### Test Case 1: Original Answer (That Was Blocked by Fast Filter)
```
"We subtract the max logit before exponentiating to avoid overflow. This shifts 
logits so the largest is 0, keeping exp() in a safe range. We upcast to float32 
for the exp/sum and then normalize by the sum along dim, matching torch.softmax 
within atol."
```

**Expected LLM Behavior:**
- Should recognize "subtract max" ‚Üí prevents overflow
- Should recognize range shift concept
- Should note missing: mathematical equivalence proof (exp(c) cancellation)
- Should provide Socratic hint about the mathematical identity

**Action:** Re-test with fast filter disabled to verify LLM can handle this

### Test Case 2: Complete Answer
```
"softmax(x) is invariant to adding a constant because both numerator and 
denominator are multiplied by exp(c), which cancels. Choosing c = ‚àímax(x) shifts 
the inputs so x ‚àí max(x) ‚àà (‚àí‚àû, 0], hence exp(x ‚àí max(x)) ‚àà (0, 1] with the 
largest term equal to 1. This keeps exponentials in a safe range while the 
denominator remains a finite positive sum. Finally, dividing by that sum yields 
a valid probability distribution identical to softmax(x)."
```

**Expected LLM Behavior:**
- Should recognize all required concepts present
- Should mark as correct
- Should provide positive feedback

**Status:** ‚úÖ Tested and working correctly

## Conclusion

**Current Prompt Quality: Good (8/10)**

The prompt provides:
- ‚úÖ Clear evaluation framework
- ‚úÖ Explicit rubric (required concepts)
- ‚úÖ Model answer for reference
- ‚úÖ Structured output format

Could be improved with:
- ‚ö†Ô∏è Module/implementation context
- ‚ö†Ô∏è Difficulty calibration
- ‚ö†Ô∏è Failure mode awareness

**Recommendation:** Current prompt is sufficient for accurate evaluation. The fast filter was too aggressive and was blocking good answers that the LLM would accept.
